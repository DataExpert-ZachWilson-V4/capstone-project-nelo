# Use the official Spark image as a base
FROM apache/spark:3.5.1-scala2.12-java17-python3-r-ubuntu

# Switch to root to install additional packages
USER root

# Add a user with ID 1001 to avoid user ID issues
RUN useradd -u 1001 -m sparkuser

# Install Python and other necessary packages, including PostgreSQL development packages
RUN apt-get update && apt-get install -y \
    python3-pip python3-dev python3-venv python-is-python3 build-essential wget curl gnupg procps libpq-dev chrony \
    && rm -rf /var/lib/apt/lists/*

# Start chrony for time synchronization
RUN service chrony start

# Install Python packages, including any that require compilation
RUN pip3 install --no-cache-dir \
    pandas \
    kafka-python \
    requests \
    qdrant-client \
    kafka-python-ng \
    six \
    xgboost \
    scikit-learn \
    bayesian-optimization \
    sentence-transformers \
    pyiceberg[hive] \
    azure-storage-blob \
    tenacity \
    python-dotenv \
    psycopg2 \
    psycopg2-binary

# Add Hadoop native libraries
RUN mkdir -p /opt/hadoop && \
    curl -sL "https://archive.apache.org/dist/hadoop/core/hadoop-3.3.4/hadoop-3.3.4.tar.gz" | tar -xz -C /opt/hadoop --strip-components=1 && \
    mv /opt/hadoop/lib/native /opt/hadoop/native

ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV YARN_CONF_DIR=$HADOOP_HOME/etc/yarn
ENV LD_LIBRARY_PATH=$HADOOP_HOME/native
ENV JAVA_HOME=/usr/local/openjdk-17

ENV SPARK_VERSION=3.5.1
ENV ICEBERG_VERSION=1.5.2

# Correct the path for JARs installation
RUN mkdir -p /opt/spark-$SPARK_VERSION-bin-hadoop3/jars && \
    wget -O /opt/spark-$SPARK_VERSION-bin-hadoop3/jars/iceberg-spark-runtime-3.3_2.12-$ICEBERG_VERSION.jar "https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.5.2/iceberg-spark-runtime-3.3_2.12-1.5.2.jar" && \
    wget -O /opt/spark-$SPARK_VERSION-bin-hadoop3/jars/spark-sql-kafka-0-10_2.12-$SPARK_VERSION.jar "https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar"

# Clean up unnecessary files to reduce image size
RUN apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Copy custom scripts from the host to the container
COPY ./Spark_streams /opt/spark/Spark_streams
COPY ./Qdrant_streams /opt/spark/Qdrant_streams
COPY ./Future_predictions_streams /opt/spark/Future_predictions_streams
COPY ./dbt /opt/spark/dbt
COPY ./spark-defaults.conf /opt/spark/conf/spark-defaults.conf
COPY ./hive/conf/hive-site.xml /opt/spark/conf/hive-site.xml
COPY ./app /app

# Change permissions to ensure scripts are executable
RUN chmod +x /opt/spark/Spark_streams/*
RUN chmod +x /opt/spark/Qdrant_streams/*
RUN chmod +x /opt/spark/Future_predictions_streams/*
RUN chmod +x /opt/spark/dbt*

# Set the correct path for Spark binaries
ENV PATH=/opt/spark-$SPARK_VERSION-bin-hadoop3/bin:$PATH

# Ensure the /opt/spark/work directory exists and is writable by sparkuser
RUN mkdir -p /opt/spark/work && chown -R sparkuser:sparkuser /opt/spark

# Switch to the new user to run Spark
USER sparkuser
